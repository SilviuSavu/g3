# g3 Configuration Example
#
# Most settings have sensible defaults. A minimal config only needs:
#
#   [providers]
#   default_provider = "anthropic.default"
#
#   [providers.anthropic.default]
#   api_key = "your-api-key"
#   model = "claude-sonnet-4-5"
#
# Everything else below is optional.

[providers]
default_provider = "anthropic.default"

# Optional: Specify different providers for each mode
# If not specified, these fall back to default_provider
# planner = "anthropic.planner"   # Provider for planning mode
# coach = "anthropic.default"     # Provider for coach in autonomous mode
# player = "anthropic.default"    # Provider for player in autonomous mode

[providers.anthropic.default]
api_key = "your-anthropic-api-key"
model = "claude-sonnet-4-5"
# max_tokens = 64000              # Optional (default: provider's max)
# temperature = 0.3               # Optional
# cache_config = "ephemeral"      # Optional: Enable prompt caching
# enable_1m_context = true        # Optional: Enable 1M context (costs extra)
# thinking_budget_tokens = 10000  # Optional: Enable extended thinking mode

# Example: A separate config for planning mode with a more capable model
# [providers.anthropic.planner]
# api_key = "your-anthropic-api-key"
# model = "claude-opus-4-5"
# thinking_budget_tokens = 16000

# Databricks provider example
# [providers.databricks.default]
# host = "https://your-workspace.cloud.databricks.com"
# model = "databricks-claude-sonnet-4"
# use_oauth = true

# OpenAI provider example
# [providers.openai.default]
# api_key = "your-openai-api-key"
# model = "gpt-4-turbo"

# OpenAI-compatible providers (OpenRouter, Groq, etc.)
# [providers.openai_compatible.openrouter]
# api_key = "your-openrouter-api-key"
# model = "anthropic/claude-3.5-sonnet"
# base_url = "https://openrouter.ai/api/v1"

# =============================================================================
# Z.ai (Zhipu AI) provider - GLM-4.7 with thinking mode
# =============================================================================
# Get API key from: https://z.ai/ (International) or https://open.bigmodel.cn/ (China)
#
# ENDPOINTS:
#   Standard API (pay-as-you-go, token-based pricing):
#     International: https://api.z.ai/api/paas/v4 (default)
#     China:         https://open.bigmodel.cn/api/paas/v4
#
#   Coding Plan API (subscription, prompt-based pricing - RECOMMENDED for g3):
#     International: https://api.z.ai/api/coding/paas/v4
#     China:         https://open.bigmodel.cn/api/coding/paas/v4
#     Pricing: Lite $3/mo (120 prompts/5hr), Pro $15/mo (600 prompts/5hr)
#     Note: Preserved thinking is ENABLED by default on coding endpoints
#
# MODELS:
#   glm-4.7        - Flagship (200K context, 128K output, ~55 tok/s)
#   glm-4.7-flashx - High-speed variant (cheaper, good balance)
#   glm-4.7-flash  - Standard variant (FREE tier available)
#
# THINKING MODE:
#   enable_thinking = true   -> Interleaved thinking (thinks before responses)
#   preserve_thinking = true -> Retains reasoning across turns
#                              (ON by default for coding endpoints)
#
# Basic configuration (Standard API - International)
# [providers.zai.default]
# api_key = "your-zai-api-key"
# model = "glm-4.7"
# # base_url = "https://api.z.ai/api/paas/v4"  # Default (International)
# enable_thinking = true
# preserve_thinking = false

# Coding Plan configuration (RECOMMENDED for AI-assisted coding)
# [providers.zai.coding]
# api_key = "your-zai-api-key"
# model = "glm-4.7"
# base_url = "https://api.z.ai/api/coding/paas/v4"
# enable_thinking = true
# preserve_thinking = true  # Enabled by default on coding endpoints

# China region configuration
# [providers.zai.china]
# api_key = "your-zai-api-key"
# model = "glm-4.7"
# base_url = "https://open.bigmodel.cn/api/paas/v4"
# enable_thinking = true

# Web search in chat (integrated with model responses)
# enable_web_search_in_chat = true
# web_search_engine = "bing"        # "bing" or "google"
# web_search_count = 10             # 1-50
# web_search_recency = "week"       # "day", "week", "month", "year"
# web_search_content_size = "medium" # "medium" or "high"

# =============================================================================
# Z.ai Standalone Tools (REST API - usable with any provider)
# =============================================================================
# These tools can be used independently of which LLM provider is active.
# They provide web search, web reading, and OCR capabilities.
#
# [zai_tools]
# enabled = true
# # api_key = "..."       # Optional: defaults to providers.zai.*.api_key
# # base_url = "..."      # Optional: defaults to https://api.z.ai/api/paas/v4
#
# Available tools when enabled:
# - zai_web_search: Search the web, returns structured results
# - zai_web_reader: Fetch URL and convert to markdown
# - zai_ocr: Extract text from images/PDFs using GLM-OCR

# =============================================================================
# Z.ai MCP Servers (Model Context Protocol)
# =============================================================================
# Connect to Z.ai's remote MCP servers for additional tool capabilities.
# MCP provides a standardized way to access tools across different AI systems.
#
# [zai_mcp.web_search]
# enabled = true
# # api_key = "..."  # Optional: falls back to zai_tools.api_key
#
# [zai_mcp.web_reader]
# enabled = true
#
# [zai_mcp.zread]
# enabled = true  # GitHub repo documentation/code access
#
# MCP Server Endpoints:
# - Web Search: https://api.z.ai/api/mcp/web_search_prime/mcp
# - Web Reader: https://api.z.ai/api/mcp/web_reader/mcp
# - Zread: https://api.z.ai/api/mcp/zread/mcp

# =============================================================================
# Embedded providers (local models via llama.cpp with Metal acceleration)
# =============================================================================
# Download models from Hugging Face:
#   huggingface-cli download bartowski/THUDM_GLM-4-32B-0414-GGUF \
#     --include "THUDM_GLM-4-32B-0414-Q6_K_L.gguf" --local-dir ~/.g3/models/
#
# GLM-4 32B - Top-tier local model for coding/reasoning (context_length auto-detected from GGUF)
# [providers.embedded.glm4]
# model_path = "~/.g3/models/THUDM_GLM-4-32B-0414-Q6_K_L.gguf"
# model_type = "glm4"            # Required: glm4, qwen, mistral, llama, codellama
# context_length = 32768         # Optional: auto-detected from GGUF (GLM-4 = 32K)
# max_tokens = 4096              # Optional: defaults to min(4096, context/4)
# temperature = 0.1
# gpu_layers = 99                # Use all GPU layers on Apple Silicon
# threads = 8

# GLM-4 9B - Smaller but very capable (minimal config - most settings auto-detected)
# [providers.embedded.glm4-9b]
# model_path = "~/.g3/models/THUDM_GLM-4-9B-0414-Q8_0.gguf"
# model_type = "glm4"
# gpu_layers = 99                # Optional but recommended for Apple Silicon

# Qwen3 4B - Small but powerful, good for ensemble usage (minimal config)
# [providers.embedded.qwen3]
# model_path = "~/.g3/models/qwen3-4b-q4_k_m.gguf"
# model_type = "qwen"
# gpu_layers = 99                # Optional but recommended for Apple Silicon

# =============================================================================
# Agent settings (all optional - these are the defaults)
# =============================================================================
# [agent]
# fallback_default_max_tokens = 8192
# enable_streaming = true
# timeout_seconds = 120
# auto_compact = true
# max_retry_attempts = 3
# autonomous_max_retry_attempts = 6
# max_context_length = 200000     # Override context window size

# =============================================================================
# Computer control (all optional - enabled by default)
# =============================================================================
# [computer_control]
# enabled = true                  # Requires OS accessibility permissions
# require_confirmation = true
# max_actions_per_second = 5

# =============================================================================
# WebDriver browser automation (all optional)
# =============================================================================
# [webdriver]
# enabled = true
# browser = "chrome-headless"     # Default. Alternative: "safari"
# chrome_binary = "/path/to/chrome"        # Optional: custom Chrome path
# chromedriver_binary = "/path/to/driver"  # Optional: custom ChromeDriver path

# =============================================================================
# Codebase Indexing Configuration
# =============================================================================
# Semantic code search using vector embeddings and hybrid search.
# Requires a running Qdrant instance: docker run -p 6334:6334 qdrant/qdrant

[index]
# Enable codebase indexing (default: false)
enabled = false

# Qdrant vector database URL
qdrant_url = "http://localhost:6334"

# Collection name in Qdrant (unique per project)
collection_name = "g3-codebase"

# -----------------------------------------------------------------------------
# Embedding Configuration
# -----------------------------------------------------------------------------
[index.embeddings]
# Provider: "openrouter" (recommended), "nvidia", or "vllm" (self-hosted)
provider = "openrouter"

# API key - use environment variable reference
# api_key = "${OPENROUTER_API_KEY}"

# Model for generating embeddings
# Qwen3-Embedding-8B: 4096 dims, #1 MTEB benchmark, $0.01/M tokens
model = "qwen/qwen3-embedding-8b"

# Vector dimensions (must match model output)
dimensions = 4096

# Base URL for self-hosted vLLM (only used when provider = "vllm")
# base_url = "http://localhost:8000/v1/embeddings"

# -----------------------------------------------------------------------------
# Search Configuration
# -----------------------------------------------------------------------------
[index.search]
# Enable hybrid search (vector + BM25 keyword matching)
# Improves recall for exact matches like function names, error codes
hybrid = true

# Weight for BM25 keyword matching (0.0-1.0)
bm25_weight = 0.3

# Weight for vector semantic matching (0.0-1.0)
vector_weight = 0.7

# Enable reranking step (slower but more accurate)
rerank = false

# -----------------------------------------------------------------------------
# Code Chunking Configuration
# -----------------------------------------------------------------------------
[index.chunking]
# Maximum tokens per chunk (larger = more context, fewer chunks)
max_chunk_tokens = 500

# Include context (file path, scope, imports) in chunk embeddings
include_context = true

# Languages to index
languages = ["rust", "python", "typescript", "javascript", "go"]

# -----------------------------------------------------------------------------
# File Watcher Configuration (Auto-indexing)
# -----------------------------------------------------------------------------
[index.watcher]
# Enable background file watching for automatic re-indexing
enabled = false

# Debounce time in milliseconds (wait before processing changes)
debounce_ms = 100

# Maximum files to process per batch
batch_size = 10

# Respect .gitignore patterns
respect_gitignore = true